{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Úkol č. 2 - Využití neuronových sítí\n",
    "\n",
    "  * **Deadline je 13. 5. 2024, 23:59:59**, pokud odevzdáte úkol do 20. 5. 2024, 23:59:59, budete penalizování -12 body, pozdější odevzdání je bez bodu.\n",
    "  * V rámci tohoto úkolu musíte sestrojit vhodný model neuronové sítě pro vícetřídou klasifikaci.\n",
    "  * Část bodů získáte za správné vypracování a část bodů získáte za výslednou přesnost Vašeho modelu na evaluačních datech.\n",
    "    \n",
    "> **Úkoly jsou zadány tak, aby Vám daly prostor pro invenci. Vymyslet _jak přesně_ budete úkol řešit, je důležitou součástí zadání a originalita či nápaditost bude také hodnocena!**\n",
    "\n",
    "Využívejte buňky typu `Markdown` k vysvětlování Vašeho postupu. Za nepřehlednost budou strhávány body.\n",
    "\n",
    "## Zdroj dat\n",
    "\n",
    " * Zdrojem dat jsou soubory `train.csv` a `evaluate.csv`.\n",
    " * Jedná se o obrázky 32x32 pixelů ve stupních šedi, které byly nějakým způsobem vyrobeny z [Fashion Mnist datasetu](https://www.kaggle.com/datasets/zalando-research/fashionmnist).\n",
    " * Soubor `train.csv` obsahuje trénovací data.\n",
    " * Cílová (vysvětlovaná) proměnná se jmenuje **label**.\n",
    " * Soubor `evaluate.csv` obsahuje testovací data bez hodnot skutečných labelů.\n",
    "\n",
    "## Pokyny k vypracování (max 18 bodů)\n",
    "\n",
    "**Body zadání**, za jejichž (poctivé) vypracování získáte **18 bodů**:\n",
    "  * V notebooku načtěte data ze souboru `train.csv`. Vhodným způsobem si je rozdělte na podmnožiny, které Vám poslouží pro trénování, porovnávání modelů a následnou predikci výkonnosti finálního modelu.\n",
    "  * Proveďte základní průzkum dat a svá pozorování diskutujte. Některé obrázky také zobrazte.\n",
    "  * Sestrojte a natrénujte několik variant modelu dopředné neuronové sítě. Přitom v rámci výpočetních možností:\n",
    "      * Okomentujte vhodnost daného modelu pro daný typ úlohy.\n",
    "      * Experimentujte s různými hloubkami a velikosmi vrstev.\n",
    "      * Experimentujte se standardizací/normalizací dat.\n",
    "      * Experimentujte s různými optimalizačními metodami.\n",
    "      * Experimentujte s různými regularizačními technikami.\n",
    "      * Získané výsledky vždy řádně okomentujte.\n",
    "\n",
    "  * Sestrojte model konvoluční neuronové sítě. Přitom v rámci výpočetních možností:\n",
    "      * Okomentujte vhodnost daného modelu pro daný typ úlohy.\n",
    "      * Experimentujte s různými hloubkami a velikosmi vrstev.\n",
    "      * Experimentujte se standardizací/normalizací dat.\n",
    "      * Experimentujte s různými optimalizačními metodami.\n",
    "      * Experimentujte s různými regularizačními technikami.\n",
    "      * Získané výsledky vždy řádně okomentujte.\n",
    "    \n",
    "  * Ze všech zkoušených možností vyberte finální model a odhadněte, jakou přesnost můžete očekávat na nových datech, která jste doposud neměli k dispozici.\n",
    "  \n",
    "  * Nakonec načtěte vyhodnocovací data ze souboru`evaluate.csv`. Pomocí finálního modelu napočítejte predikce pro tyto data (vysvětlovaná proměnná v nich již není). Vytvořte soubor `results.csv`, ve kterém získané predikce uložíte do sloupce **label** a identifikátory do sloupce **ID**. Tento soubor též odevzdejte (uložte do projektu vedle notebooku).\n",
    "   \n",
    "   * Ukázka prvních řádků souboru `results.csv`:\n",
    "  \n",
    "```\n",
    "ID,label\n",
    "0,0\n",
    "1,1\n",
    "...\n",
    "```\n",
    "\n",
    "## Vyhodnocovací část (max 7 bodů)\n",
    "Za přesnost (accuraccy) na odevzdaných predikcích pro vyhodnocovací množnu získáte dalších max **7 bodů**.\n",
    "\n",
    "Označíme-li $A$ přesnost, které jste dosáhli, zaokrouhlenou na 2 desetinná místa, akumulují se výsledné body podle následujících pravidel:\n",
    "* pokud $A \\geq 0.80$ obdržíte +1 bod\n",
    "* pokud $A \\geq 0.83$ obdržíte +1 bod\n",
    "* pokud $A \\geq 0.86$ obdržíte +1 bod\n",
    "* pokud $A \\geq 0.87$ obdržíte +1 bod\n",
    "* pokud $A \\geq 0.88$ obdržíte +1 bod\n",
    "* pokud $A \\geq 0.89$ obdržíte +1 bod\n",
    "* pokud $A \\geq 0.90$ obdržíte +1 bod\n",
    "\n",
    "**Příklad:** Pokud bude Vaše přesnost 0.856, vyjde A = 0.86 a vy získáte 3 body.\n",
    "\n",
    "\n",
    "## Poznámky k odevzdání\n",
    "\n",
    "  * Řiďte se pokyny ze stránky https://courses.fit.cvut.cz/BI-ML2/homeworks/index.html.\n",
    "  * Vytvořte i csv soubor `results.csv` s predikcemi a uložte ho v rámci projektu vedle ipython notebooku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### odtud už je to Vaše\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŘEŠENÍ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Předzpracování dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importuju potřebné knihovny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Načítam si dataset ze csv-souboru a podívám se na informace o něm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52500, 1025)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>pix10</th>\n",
       "      <th>...</th>\n",
       "      <th>pix1016</th>\n",
       "      <th>pix1017</th>\n",
       "      <th>pix1018</th>\n",
       "      <th>pix1019</th>\n",
       "      <th>pix1020</th>\n",
       "      <th>pix1021</th>\n",
       "      <th>pix1022</th>\n",
       "      <th>pix1023</th>\n",
       "      <th>pix1024</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pix1  pix2  pix3  pix4  pix5  pix6  pix7  pix8  pix9  pix10  ...  pix1016  \\\n",
       "0     0     0     0     0     0     0     0     0     0      0  ...        0   \n",
       "1     1     1     1     1     1     1     1     1     1      1  ...        1   \n",
       "2     1     1     1     1     1     1     1     1     1      1  ...        1   \n",
       "3     0     0     0     0     0     0     0     0     0      0  ...        0   \n",
       "4     1     1     1     1     1     1     1     1     1      1  ...        1   \n",
       "\n",
       "   pix1017  pix1018  pix1019  pix1020  pix1021  pix1022  pix1023  pix1024  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        1        1        1        1        1        1        1        1   \n",
       "2        1        1        1        1        1        1        1        1   \n",
       "3        0        0        0        0        0        0        0        0   \n",
       "4        1        1        1        1        1        1        1        1   \n",
       "\n",
       "   label  \n",
       "0      3  \n",
       "1      3  \n",
       "2      7  \n",
       "3      9  \n",
       "4      5  \n",
       "\n",
       "[5 rows x 1025 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52500 entries, 0 to 52499\n",
      "Columns: 1025 entries, pix1 to label\n",
      "dtypes: int64(1025)\n",
      "memory usage: 410.6 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pix1       12\n",
       "pix2       12\n",
       "pix3       12\n",
       "pix4       12\n",
       "pix5       12\n",
       "           ..\n",
       "pix1021    12\n",
       "pix1022    12\n",
       "pix1023    12\n",
       "pix1024    12\n",
       "label      10\n",
       "Length: 1025, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>pix10</th>\n",
       "      <th>...</th>\n",
       "      <th>pix1016</th>\n",
       "      <th>pix1017</th>\n",
       "      <th>pix1018</th>\n",
       "      <th>pix1019</th>\n",
       "      <th>pix1020</th>\n",
       "      <th>pix1021</th>\n",
       "      <th>pix1022</th>\n",
       "      <th>pix1023</th>\n",
       "      <th>pix1024</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "      <td>52500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.877390</td>\n",
       "      <td>0.877390</td>\n",
       "      <td>0.877390</td>\n",
       "      <td>0.877390</td>\n",
       "      <td>0.877410</td>\n",
       "      <td>0.877429</td>\n",
       "      <td>0.877429</td>\n",
       "      <td>0.877543</td>\n",
       "      <td>0.878038</td>\n",
       "      <td>0.880648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929867</td>\n",
       "      <td>0.909143</td>\n",
       "      <td>0.893524</td>\n",
       "      <td>0.883505</td>\n",
       "      <td>0.878952</td>\n",
       "      <td>0.877467</td>\n",
       "      <td>0.877410</td>\n",
       "      <td>0.877390</td>\n",
       "      <td>0.877390</td>\n",
       "      <td>4.509752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.493605</td>\n",
       "      <td>1.493605</td>\n",
       "      <td>1.493605</td>\n",
       "      <td>1.493605</td>\n",
       "      <td>1.493601</td>\n",
       "      <td>1.493596</td>\n",
       "      <td>1.493596</td>\n",
       "      <td>1.493682</td>\n",
       "      <td>1.494258</td>\n",
       "      <td>1.528912</td>\n",
       "      <td>...</td>\n",
       "      <td>2.835341</td>\n",
       "      <td>2.224631</td>\n",
       "      <td>1.794160</td>\n",
       "      <td>1.615415</td>\n",
       "      <td>1.505722</td>\n",
       "      <td>1.493854</td>\n",
       "      <td>1.493677</td>\n",
       "      <td>1.493605</td>\n",
       "      <td>1.493605</td>\n",
       "      <td>2.872106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pix1          pix2          pix3          pix4          pix5  \\\n",
       "count  52500.000000  52500.000000  52500.000000  52500.000000  52500.000000   \n",
       "mean       0.877390      0.877390      0.877390      0.877390      0.877410   \n",
       "std        1.493605      1.493605      1.493605      1.493605      1.493601   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "max       11.000000     11.000000     11.000000     11.000000     11.000000   \n",
       "\n",
       "               pix6          pix7          pix8          pix9         pix10  \\\n",
       "count  52500.000000  52500.000000  52500.000000  52500.000000  52500.000000   \n",
       "mean       0.877429      0.877429      0.877543      0.878038      0.880648   \n",
       "std        1.493596      1.493596      1.493682      1.494258      1.528912   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "max       11.000000     11.000000     11.000000     11.000000     73.000000   \n",
       "\n",
       "       ...       pix1016       pix1017       pix1018       pix1019  \\\n",
       "count  ...  52500.000000  52500.000000  52500.000000  52500.000000   \n",
       "mean   ...      0.929867      0.909143      0.893524      0.883505   \n",
       "std    ...      2.835341      2.224631      1.794160      1.615415   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "max    ...    187.000000    172.000000    134.000000    116.000000   \n",
       "\n",
       "            pix1020       pix1021       pix1022       pix1023       pix1024  \\\n",
       "count  52500.000000  52500.000000  52500.000000  52500.000000  52500.000000   \n",
       "mean       0.878952      0.877467      0.877410      0.877390      0.877390   \n",
       "std        1.505722      1.493854      1.493677      1.493605      1.493605   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "max       41.000000     11.000000     11.000000     11.000000     11.000000   \n",
       "\n",
       "              label  \n",
       "count  52500.000000  \n",
       "mean       4.509752  \n",
       "std        2.872106  \n",
       "min        0.000000  \n",
       "25%        2.000000  \n",
       "50%        5.000000  \n",
       "75%        7.000000  \n",
       "max        9.000000  \n",
       "\n",
       "[8 rows x 1025 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "display(df.shape)\n",
    "display(df.head())\n",
    "display(df.info())\n",
    "display(df.nunique())\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pix1', 'pix2', 'pix3', 'pix4', 'pix5', 'pix6', 'pix7', 'pix8', 'pix9',\n",
       "       'pix10',\n",
       "       ...\n",
       "       'pix1016', 'pix1017', 'pix1018', 'pix1019', 'pix1020', 'pix1021',\n",
       "       'pix1022', 'pix1023', 'pix1024', 'label'],\n",
       "      dtype='object', length=1025)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pix1       0\n",
       "pix2       0\n",
       "pix3       0\n",
       "pix4       0\n",
       "pix5       0\n",
       "          ..\n",
       "pix1021    0\n",
       "pix1022    0\n",
       "pix1023    0\n",
       "pix1024    0\n",
       "label      0\n",
       "Length: 1025, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak my můžeme vidět náš dataset neobsahuje žádná nullová data. Proto můžu klidně pracovat s daty dále."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobrazím několik prvních obrázků z našeho datasetu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(4, 4, figsize=(8,8))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for image_index, ax in enumerate(axs):\n",
    "    image_pixels = df.iloc[image_index, 1:].values\n",
    "    # Reshape the pixel values into a 32x32 image\n",
    "    image = image_pixels.reshape(32,32)\n",
    "    label_image = df['label'][image_index]\n",
    "    \n",
    "    ax.imshow(image, cmap='gray', extent=[0, 1, 0, 1])\n",
    "    ax.set_title(f\"Label: {label_image}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Použítí modelů machine learningu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II(a). Rozdělení dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nejprve, podívám se, je-li grafická karta k dispozici. Pokud ano, tak budeme trénovat pomocí ní, jinak pomocí procesoru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozdělím si dataset na cílovou proměnnou (v našem případě to je 'Label'), kterou chci predikovat, a na matici pixelů, pomocí kterou budu predikovat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xdata = df.drop('label', axis=1)\n",
    "ydata = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ted' už rozdělím data na trénovací a testovací množiny v poměru 60:40."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31500, 1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(31500,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(21000, 1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(21000,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rd_seed = 100\n",
    "\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(Xdata, ydata, test_size=0.4, random_state=rd_seed) \n",
    "display(Xtrain.shape)\n",
    "display(ytrain.shape)\n",
    "display(Xtest.shape)\n",
    "display(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozdělím testovací množinu na validační a testovací."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10500, 1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10500,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10500, 1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10500,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xval, Xtest, yval, ytest = train_test_split(Xtest, ytest, test_size=0.5, random_state=rd_seed) \n",
    "display(Xval.shape)\n",
    "display(yval.shape)\n",
    "display(Xtest.shape)\n",
    "display(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applikuju metody standardizace a normalizace dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_standard = StandardScaler()\n",
    "Xtrain_standard = scaler_standard.fit_transform(Xtrain)\n",
    "Xval_standard = scaler_standard.transform(Xval)\n",
    "Xtest_standard = scaler_standard.transform(Xtest)\n",
    "\n",
    "scaler_min_max = MinMaxScaler()\n",
    "Xtrain_min_max = scaler_min_max.fit_transform(Xtrain)\n",
    "Xval_min_max = scaler_min_max.transform(Xval)\n",
    "Xtest_min_max = scaler_min_max.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvořím tensory a dataloadery našich dat pro snadné trénování neuronových sítí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_tensor_dataset(X, y, device=torch.device(device)):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float).to(device)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return dataset\n",
    "\n",
    "def create_data_loaders(train_dataset, val_dataset, test_dataset, batch_size_train=64, batch_size_val=128, batch_size_test=128):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_tensor_dataset(Xtrain.values.reshape(-1, 1, 32, 32), ytrain.values, device)\n",
    "val_dataset = create_tensor_dataset(Xval.values.reshape(-1, 1, 32, 32), yval.values, device)\n",
    "test_dataset = create_tensor_dataset(Xtest.values.reshape(-1, 1, 32, 32), ytest.values, device)\n",
    "\n",
    "train_loader, val_loader, test_loader = create_data_loaders(train_dataset, val_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_stand = create_tensor_dataset(Xtrain_standard.reshape(-1, 1, 32, 32), ytrain.values, device)\n",
    "val_dataset_stand = create_tensor_dataset(Xval_standard.reshape(-1, 1, 32, 32), yval.values, device)\n",
    "test_dataset_stand = create_tensor_dataset(Xtest_standard.reshape(-1, 1, 32, 32), ytest.values, device)\n",
    "\n",
    "train_loader_stand, val_loader_stand, test_loader_stand = create_data_loaders(train_dataset_stand, val_dataset_stand, test_dataset_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_min_max = create_tensor_dataset(Xtrain_min_max.reshape(-1, 1, 32, 32), ytrain.values, device)\n",
    "val_dataset_min_max = create_tensor_dataset(Xval_min_max.reshape(-1, 1, 32, 32), yval.values, device)\n",
    "test_dataset_min_max = create_tensor_dataset(Xtest_min_max.reshape(-1, 1, 32, 32), ytest.values, device)\n",
    "\n",
    "train_loader_min_max, val_loader_min_max, test_loader_min_max = create_data_loaders(train_dataset_min_max, val_dataset_min_max, test_dataset_min_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvořím slovník pro naše modely a metody ladění modelů"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = {\n",
    "    'MLP': [],\n",
    "    'CNN': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II(b). Implementace a použití modelů FeedForward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Použiju model dopředné neuronové sítě a budu ho ladit pomocí různých technik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(b). Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nejprve, zkusím model dopředné neuronové sítě na původních datech a zároven vyladím model na různých hloubkách a velikostech vrstev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvořím dávky dat pro snadné trénování a menší zatížení paměti grafické karty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvořím mřížku parametrů pro ladění dopředné neuronové síti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = [\n",
    "    {'hidden_layers': [64, 32], 'output_size': 10},  # Network with 2 hidden layers, 64 and 32 units\n",
    "    {'hidden_layers': [128, 64, 32], 'output_size': 10},  # Network with 3 hidden layers, 128, 64, and 32 units\n",
    "    {'hidden_layers': [256, 128, 64], 'output_size': 20},\n",
    "    {'hidden_layers': [512, 256, 128, 64], 'output_size': 20},\n",
    "    {'hidden_layers': [1024, 512, 256, 128], 'output_size': 40}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napíšu třídu dopředné neuronové síti s nastavitelnou hloubkou a velikostmi každou vrstvy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layers[0])\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers)-1)])\n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(32*32, [64,32], 10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32]) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(pixels)\n",
    "print(pixels.shape, outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Použiju kategorickou relativní entropii a kouknu se na průměrnou chybu predikcí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 8.4452486038208\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "outputs = model(pixels)\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako první optimizer použiju algoritmus Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napíšu funkci pro trénování jedne epochy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loss_fn, optimizer, train_loader):\n",
    "    running_cum_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "    \n",
    "        # Reshape the pixels to match the input size expected by the model\n",
    "        inputs = inputs.view(-1, 32, 32)  # Assuming input size is 32x32\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_cum_loss += loss.item() * inputs.shape[0]\n",
    "            \n",
    "    return running_cum_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podívám se na celkovou chybu predikcí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.0360088359212118\n"
     ]
    }
   ],
   "source": [
    "loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ted, už začnu trénování modelů na různých parametrech na 15 epochách s technikou předčasné zastavení, zjistím nejlepší parametry modelu na původních datech a uložím model do složky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_mlp = {}\n",
    "\n",
    "# Loop through each network configuration\n",
    "for index, network in enumerate(networks):\n",
    "    input_size = 32 * 32  # Input image size\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    hidden_layers = network['hidden_layers']  # Hidden layer sizes\n",
    "    \n",
    "    # Create an instance of MyNet with the specified architecture\n",
    "    model = MLP(input_size, hidden_layers, output_size).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):    \n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset)\n",
    "        vacc = vcorrect / len(val_dataset)\n",
    "    \n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_mlp.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_mlp[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.506, 1: 0.48, 3: 0.454}\n",
      "{'hidden_layers': [512, 256, 128, 64], 'output_size': 20}\n",
      "0.454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_mlp)\n",
    "index = min(best_mlp, key=best_mlp.get)\n",
    "print(networks[index])\n",
    "print(best_mlp[index])\n",
    "\n",
    "model_path = \"saves/best_model_mlp.pt\"\n",
    "# načteme nejlepší model\n",
    "model_mlp = MLP(input_size=32*32, hidden_layers=networks[index]['hidden_layers'], output_size=networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_mlp.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jsou to nejlepší parametry modelu dopředné neuronové síti na originálních datech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podívám se na predikce pravděpodobností"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "tensor([3, 6, 1, 2, 0, 9, 7, 5, 8, 1, 9, 8, 3, 2, 9, 9, 6, 2, 7, 3, 8, 9, 2, 0,\n",
      "        2, 5, 2, 5, 9, 9, 5, 3, 8, 0, 3, 0, 7, 0, 3, 3, 7, 1, 7, 7, 3, 1, 1, 9,\n",
      "        3, 7, 1, 4, 5, 1, 0, 6, 3, 6, 2, 5, 9, 3, 5, 0], device='cuda:0')\n",
      "tensor([3, 6, 1, 6, 0, 9, 7, 5, 8, 1, 9, 8, 3, 6, 9, 9, 6, 2, 7, 3, 8, 9, 2, 0,\n",
      "        2, 5, 4, 5, 9, 9, 5, 6, 8, 0, 3, 0, 7, 0, 3, 3, 7, 1, 7, 7, 3, 1, 1, 9,\n",
      "        4, 7, 1, 4, 5, 1, 0, 6, 3, 6, 2, 5, 9, 3, 5, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_mlp(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_mlp(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podívám se na přesnost na validačních datech a matici záměn a uložím přesnost tohoto modelu do slovníku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8404761904761905\n",
      "[[ 952    3   24   48    2    0   41    0   10    0]\n",
      " [  11 1059    3   16    2    0    1    0    0    0]\n",
      " [   7    7  804   11  112    0   79    0   16    0]\n",
      " [  59   12   19  856   53    0   10    0    5    0]\n",
      " [   4    2  151   29  687    0  105    0   11    0]\n",
      " [   1    3    0    0    0  959    0   69    6   42]\n",
      " [ 273    3  140   34   62    1  492    0   24    0]\n",
      " [   0    0    0    0    0   18    0  960    1   63]\n",
      " [   4    1    8    7    3    3    8    5 1046    0]\n",
      " [   0    1    1    1    0    8    0   31    1 1010]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader.dataset))\n",
    "val_y = np.zeros(len(val_loader.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_mlp(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['MLP'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476], 'CNN': []}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(b). Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udělám předchozí procedury trénování modelů, ale ted už na standardních datech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader_stand)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 2.3410532474517822\n",
      "Total training loss: 0.7822901890391395\n"
     ]
    }
   ],
   "source": [
    "model = MLP(32*32, [64,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "outputs = model(pixels)\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss = train_one_epoch(model, loss_fn, optimizer, train_loader_stand)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_mlp_stand = {}\n",
    "\n",
    "# Loop through each network configuration\n",
    "for index, network in enumerate(networks):\n",
    "    input_size = 32 * 32  # Input image size\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    hidden_layers = network['hidden_layers']  # Hidden layer sizes\n",
    "    \n",
    "    # Create an instance of MyNet with the specified architecture\n",
    "    model = MLP(input_size, hidden_layers, output_size).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch(model, loss_fn, optimizer, train_loader_stand)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader_stand):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset_stand)\n",
    "        vacc = vcorrect / len(val_dataset_stand)\n",
    "    \n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_mlp_stand.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_mlp_stand[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.485, 1: 0.452}\n",
      "{'hidden_layers': [128, 64, 32], 'output_size': 10}\n",
      "0.452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_mlp_stand)\n",
    "index = min(best_mlp_stand, key=best_mlp_stand.get)\n",
    "print(networks[index])\n",
    "print(best_mlp_stand[index])\n",
    "\n",
    "model_path = \"saves/best_model_mlp_stand.pt\"\n",
    "# načteme nejlepší model\n",
    "model_mlp_stand = MLP(input_size=32*32, hidden_layers=networks[index]['hidden_layers'], output_size=networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_mlp_stand.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "tensor([4, 4, 7, 9, 1, 5, 9, 9, 0, 5, 2, 0, 3, 2, 4, 6, 2, 7, 3, 7, 2, 2, 8, 7,\n",
      "        9, 8, 6, 5, 9, 1, 5, 4, 1, 5, 9, 5, 3, 3, 6, 5, 5, 4, 8, 1, 2, 2, 5, 7,\n",
      "        8, 1, 8, 1, 5, 1, 0, 5, 5, 5, 6, 9, 4, 2, 9, 9], device='cuda:0')\n",
      "tensor([4, 4, 7, 9, 1, 5, 9, 9, 0, 7, 2, 0, 3, 2, 4, 2, 2, 7, 3, 7, 2, 2, 8, 7,\n",
      "        9, 8, 0, 5, 9, 1, 5, 3, 1, 5, 9, 5, 3, 3, 6, 5, 5, 4, 8, 1, 6, 2, 5, 7,\n",
      "        8, 1, 8, 1, 5, 1, 0, 5, 5, 5, 2, 9, 4, 2, 9, 9], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_mlp_stand(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_mlp_stand(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8372380952380952\n",
      "[[ 901    3   12   69    9    4   72    0   10    0]\n",
      " [   2 1058    3   17    7    0    5    0    0    0]\n",
      " [  12    1  748    9  163    0   81    0   22    0]\n",
      " [  39   20    6  864   57    0   24    0    4    0]\n",
      " [   1    1   79   32  796    0   74    0    6    0]\n",
      " [   2    0   12    0    0  957    2   68   10   29]\n",
      " [ 188    6   92   48  128    0  542    0   24    1]\n",
      " [   0    0    0    0    0   72    0  923    4   43]\n",
      " [   8    0    5    8    9    8   10    5 1031    1]\n",
      " [   0    0    0    1    0   15    0   59    7  971]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader_stand.dataset))\n",
    "val_y = np.zeros(len(val_loader_stand.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader_stand:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_mlp_stand(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['MLP'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Přesnost se trochu zmenšila, ale budu dál zkoušet různé techniky předzpracování dat, ladění modelů neuronových sítí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476, 0.837238], 'CNN': []}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(b). MinMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Udělám předchozí procedury trénování modelů, ale ted zkusím na datech s MinMax normalizací"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader_min_max)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 2.367041826248169\n",
      "Total training loss: 0.982072115186661\n"
     ]
    }
   ],
   "source": [
    "model = MLP(32*32, [64,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "outputs = model(pixels)\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss = train_one_epoch(model, loss_fn, optimizer, train_loader_min_max)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_mlp_min_max = {}\n",
    "\n",
    "# Loop through each network configuration\n",
    "for index, network in enumerate(networks):\n",
    "    input_size = 32 * 32  # Input image size\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    hidden_layers = network['hidden_layers']  # Hidden layer sizes\n",
    "    \n",
    "    # Create an instance of MyNet with the specified architecture\n",
    "    model = MLP(input_size, hidden_layers, output_size).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch(model, loss_fn, optimizer, train_loader_min_max)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader_min_max):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset_min_max)\n",
    "        vacc = vcorrect / len(val_dataset_min_max)\n",
    "    \n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_mlp_min_max.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_mlp_min_max[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.45}\n",
      "{'hidden_layers': [64, 32], 'output_size': 10}\n",
      "0.45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_mlp_min_max)\n",
    "index = min(best_mlp_min_max, key=best_mlp_min_max.get)\n",
    "print(networks[index])\n",
    "print(best_mlp_min_max[index])\n",
    "\n",
    "model_path = \"saves/best_model_mlp_min_max.pt\"\n",
    "# načteme nejlepší model\n",
    "model_mlp_min_max = MLP(input_size=32*32, hidden_layers=networks[index]['hidden_layers'], output_size=networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_mlp_min_max.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
      "         True,  True,  True, False,  True,  True,  True,  True, False,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "         True,  True, False, False,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "tensor([6, 4, 3, 5, 5, 2, 6, 9, 0, 6, 1, 5, 4, 3, 4, 7, 6, 6, 4, 3, 4, 1, 4, 7,\n",
      "        7, 7, 5, 9, 4, 5, 0, 5, 9, 5, 7, 2, 1, 7, 3, 9, 9, 5, 2, 6, 5, 3, 3, 9,\n",
      "        4, 9, 9, 3, 2, 6, 0, 7, 7, 4, 7, 3, 5, 7, 4, 3], device='cuda:0')\n",
      "tensor([6, 4, 3, 5, 5, 2, 6, 7, 0, 6, 1, 5, 4, 0, 4, 7, 6, 6, 6, 3, 4, 1, 4, 7,\n",
      "        7, 7, 5, 9, 4, 5, 0, 5, 9, 5, 7, 2, 1, 7, 0, 9, 9, 5, 6, 2, 5, 3, 3, 9,\n",
      "        4, 9, 9, 3, 2, 6, 0, 7, 7, 6, 7, 3, 5, 7, 4, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_mlp_min_max(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_mlp_min_max(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8368571428571429\n",
      "[[ 885    8   17   56    4    2  100    0    8    0]\n",
      " [   2 1066    1   15    1    0    7    0    0    0]\n",
      " [  10    8  714   12  156    0  122    0   14    0]\n",
      " [  43   28   14  853   44    0   30    0    2    0]\n",
      " [   6    5   69   32  781    0   94    0    2    0]\n",
      " [   1    0    0    0    0  959    1   82    4   33]\n",
      " [ 167    8   96   44   99    0  597    0   18    0]\n",
      " [   0    0    0    0    0   49    0  932    1   60]\n",
      " [   5    0    8   11    7    9   13    5 1025    2]\n",
      " [   1    4    0    0    1   21    0   51    0  975]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader_min_max.dataset))\n",
    "val_y = np.zeros(len(val_loader_min_max.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader_min_max:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_mlp_min_max(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['MLP'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Přesnost je skoro stejná jako na datech se standardizací, ale je také menší než na původnch datech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476, 0.837238, 0.836857], 'CNN': []}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(b). Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ted zkusím dropout regularizaci. Napíšu novou třídu s implementací atributu dropout. A pak provedu té samé procedury trénování modelů"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDropout(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size, dropout_p=0.1):\n",
    "        super(MLPDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layers[0])\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers)-1)])\n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "model = MLPDropout(32*32, [64,32], 10, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 12.104911804199219\n",
      "Total training loss: 1.1958602135597713\n"
     ]
    }
   ],
   "source": [
    "model = MLPDropout(32*32, [64,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "outputs = model(pixels)\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_mlp_drop = {}\n",
    "\n",
    "# Loop through each network configuration\n",
    "for index, network in enumerate(networks):\n",
    "    input_size = 32 * 32  # Input image size\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    hidden_layers = network['hidden_layers']  # Hidden layer sizes\n",
    "    \n",
    "    # Create an instance of MyNet with the specified architecture\n",
    "    model = MLPDropout(input_size, hidden_layers, output_size, 0.1).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):    \n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset)\n",
    "        vacc = vcorrect / len(val_dataset)\n",
    "    \n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_mlp_drop.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_mlp_drop[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.545, 1: 0.47}\n",
      "{'hidden_layers': [128, 64, 32], 'output_size': 10}\n",
      "0.47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_mlp_drop)\n",
    "index = min(best_mlp_drop, key=best_mlp_drop.get)\n",
    "print(networks[index])\n",
    "print(best_mlp_drop[index])\n",
    "\n",
    "model_path = \"saves/best_model_mlp_drop.pt\"\n",
    "# načteme nejlepší model\n",
    "model_mlp_drop = MLPDropout(input_size=32*32, \n",
    "                            hidden_layers=networks[index]['hidden_layers'], \n",
    "                            output_size=networks[index]['output_size'], \n",
    "                            dropout_p=0.1).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_mlp_drop.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False,  True,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True, False, False,  True,  True,  True, False,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "tensor([4, 1, 5, 5, 3, 4, 1, 3, 7, 5, 0, 6, 8, 8, 6, 5, 4, 5, 7, 7, 0, 5, 1, 0,\n",
      "        3, 0, 8, 1, 3, 8, 8, 9, 9, 6, 1, 1, 3, 2, 0, 0, 8, 7, 7, 1, 6, 0, 8, 6,\n",
      "        5, 2, 2, 8, 3, 0, 3, 0, 7, 9, 2, 5, 6, 7, 8, 9], device='cuda:0')\n",
      "tensor([4, 1, 5, 5, 3, 4, 1, 3, 7, 5, 0, 4, 8, 8, 6, 7, 4, 5, 7, 7, 0, 5, 1, 0,\n",
      "        3, 0, 8, 1, 4, 8, 8, 9, 9, 0, 1, 1, 3, 6, 0, 0, 8, 7, 7, 1, 4, 3, 8, 6,\n",
      "        5, 2, 6, 8, 3, 0, 3, 0, 7, 9, 2, 5, 6, 7, 8, 9], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_mlp_drop(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_mlp_drop(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8199047619047619\n",
      "[[ 987    3   12   47    1    1   25    0    4    0]\n",
      " [  13 1032    1   38    4    0    2    0    0    2]\n",
      " [  44    4  765   10  128    0   76    0    9    0]\n",
      " [  95   20    3  837   27    1   27    0    4    0]\n",
      " [   5    0  138   82  680    0   81    0    3    0]\n",
      " [   1    0    0    1    0  964    1   61   10   42]\n",
      " [ 310    4  136   35   96    1  426    0   21    0]\n",
      " [   0    0    0    0    0   47    0  915    7   73]\n",
      " [  14    0   12   13    4   10   13    8 1009    2]\n",
      " [   1    0    1    0    0   16    0   41    0  994]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader.dataset))\n",
    "val_y = np.zeros(len(val_loader.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_mlp_drop(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['MLP'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bohužel, dropout regularizace ještě víc zhoršila přesnost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476, 0.837238, 0.836857, 0.819905], 'CNN': []}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(b). Dropout + L1reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusím kombinaci regularizací Dropout a L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32]) torch.Size([64, 10])\n",
      "Total loss for this batch: 10.525484085083008\n"
     ]
    }
   ],
   "source": [
    "model = MLPDropout(32*32, [64,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "print(pixels.shape, outputs.shape)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "outputs = model(pixels)\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro trénování modelů napíšu novou funkci s přidáním L1 regularizace, ale stále budu používat instanci nové třídy Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_l1reg(model, loss_fn, optimizer, train_loader, l1_lambda=0.1):\n",
    "    running_cum_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "    \n",
    "        # Reshape the pixels to match the input size expected by the model\n",
    "        inputs = inputs.view(-1, 32, 32)  # Assuming input size is 32x32\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        l1_reg = torch.tensor(0., device='cuda:0')\n",
    "        l1_reg += torch.linalg.vector_norm(model.output.weight.flatten(),1)\n",
    "\n",
    "        loss = loss + l1_lambda*l1_reg\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_cum_loss += loss.item() * inputs.shape[0]\n",
    "            \n",
    "    return running_cum_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celková trénovací chyba: 2.1898035631028434\n"
     ]
    }
   ],
   "source": [
    "loss = train_one_epoch_l1reg(model, loss_fn, optimizer, train_loader)\n",
    "print(f\"Celková trénovací chyba: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_mlp_drop_l1reg = {}\n",
    "\n",
    "# Loop through each network configuration\n",
    "for index, network in enumerate(networks):\n",
    "    input_size = 32 * 32  # Input image size\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    hidden_layers = network['hidden_layers']  # Hidden layer sizes\n",
    "    \n",
    "    # Create an instance of MyNet with the specified architecture\n",
    "    model = MLPDropout(input_size, hidden_layers, output_size, 0.1).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 3\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch_l1reg(model, loss_fn, optimizer, train_loader)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset)\n",
    "        vacc = vcorrect / len(val_dataset)\n",
    "\n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_mlp_drop_l1reg.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_mlp_drop_l1reg[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.501}\n",
      "{'hidden_layers': [64, 32], 'output_size': 10}\n",
      "0.501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_mlp_drop_l1reg)\n",
    "index = min(best_mlp_drop_l1reg, key=best_mlp_drop_l1reg.get)\n",
    "print(networks[index])\n",
    "print(best_mlp_drop_l1reg[index])\n",
    "\n",
    "model_path = \"saves/best_model_mlp_drop_l1reg.pt\"\n",
    "# načteme nejlepší model\n",
    "model_mlp_drop_l1reg = MLPDropout(input_size=32*32, \n",
    "                                  hidden_layers=networks[index]['hidden_layers'], \n",
    "                                  output_size=networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_mlp_drop_l1reg.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True, False,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         True, False,  True, False,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False, False,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "tensor([3, 4, 7, 3, 7, 6, 3, 2, 5, 0, 3, 3, 5, 4, 4, 3, 3, 7, 9, 3, 4, 7, 1, 8,\n",
      "        4, 5, 5, 7, 9, 2, 9, 5, 6, 3, 6, 4, 0, 6, 7, 7, 9, 5, 0, 8, 0, 2, 9, 6,\n",
      "        4, 4, 9, 7, 3, 1, 4, 4, 0, 6, 7, 0, 9, 6, 7, 3], device='cuda:0')\n",
      "tensor([3, 2, 7, 3, 7, 3, 3, 2, 5, 6, 3, 4, 5, 2, 4, 6, 3, 7, 9, 3, 4, 7, 1, 8,\n",
      "        4, 5, 5, 5, 9, 2, 9, 5, 6, 3, 6, 4, 0, 0, 7, 7, 9, 5, 0, 8, 6, 4, 9, 6,\n",
      "        4, 2, 9, 7, 3, 1, 4, 4, 0, 0, 7, 0, 9, 6, 7, 4], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_mlp_drop_l1reg(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_mlp_drop_l1reg(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8025714285714286\n",
      "[[ 865    6   28   94    4    1   67    0   14    1]\n",
      " [   4 1049    3   23    3    0    9    0    1    0]\n",
      " [   5    6  585   11  275    1  144    0    9    0]\n",
      " [  54   31   12  832   59    1   21    0    4    0]\n",
      " [   2    4   41   41  802    0   92    0    7    0]\n",
      " [   0    1    4    0    0  937    0   80    7   51]\n",
      " [ 208    6  117   52  169    0  448    1   28    0]\n",
      " [   0    0    0    0    0   31    0  911    2   98]\n",
      " [   2    1    9    5   10   11   31   13 1002    1]\n",
      " [   1    1    1    2    0   15    1   35    1  996]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader.dataset))\n",
    "val_y = np.zeros(len(val_loader.dataset))\n",
    "\n",
    "ii = 0\n",
    "for vdata in val_loader:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_mlp_drop_l1reg(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['MLP'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kombinace dvou regularizací Dropout a L1 zhoršila přesnost ještě víc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476, 0.837238, 0.836857, 0.819905, 0.802571], 'CNN': []}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(b). Dropout + L2reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tedy, podívám se jak to bude s L2 regularizací namísto L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32]) torch.Size([64, 10])\n",
      "Total loss for this batch: 16.026247024536133\n"
     ]
    }
   ],
   "source": [
    "model = MLPDropout(32*32, [64,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "print(pixels.shape, outputs.shape)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "outputs = model(pixels)\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napíšu novou funkci trénování s L2 regularizací"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_l2reg(model, loss_fn, optimizer, train_loader, l2_lambda=0.1):\n",
    "    running_cum_loss = 0.\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "    \n",
    "        # Reshape the pixels to match the input size expected by the model\n",
    "        inputs = inputs.view(-1, 32, 32)  # Assuming input size is 32x32\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Calculate L2 regularization term\n",
    "        l2_reg = torch.tensor(0., device='cuda:0')\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param, 2)**2\n",
    "        \n",
    "        # Add L2 regularization term to the loss\n",
    "        loss += 0.5 * l2_lambda * l2_reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_cum_loss += loss.item() * inputs.shape[0]\n",
    "            \n",
    "    return running_cum_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celková trénovací chyba: 1.9727412394417656\n"
     ]
    }
   ],
   "source": [
    "loss = train_one_epoch_l2reg(model, loss_fn, optimizer, train_loader)\n",
    "print(f\"Celková trénovací chyba: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_mlp_drop_l2reg = {}\n",
    "\n",
    "# Loop through each network configuration\n",
    "for index, network in enumerate(networks):\n",
    "    input_size = 32 * 32  # Input image size\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    hidden_layers = network['hidden_layers']  # Hidden layer sizes\n",
    "    \n",
    "    # Create an instance of MyNet with the specified architecture\n",
    "    model = MLPDropout(input_size, hidden_layers, output_size, 0.1).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):    \n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch_l2reg(model, loss_fn, optimizer, train_loader)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset)\n",
    "        vacc = vcorrect / len(val_dataset)\n",
    "\n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_mlp_drop_l2reg.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_mlp_drop_l2reg[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.639}\n",
      "{'hidden_layers': [64, 32], 'output_size': 10}\n",
      "0.639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_mlp_drop_l2reg)\n",
    "index = min(best_mlp_drop_l2reg, key=best_mlp_drop_l2reg.get)\n",
    "print(networks[index])\n",
    "print(best_mlp_drop_l2reg[index])\n",
    "\n",
    "model_path = \"saves/best_model_mlp_drop_l2reg.pt\"\n",
    "# načteme nejlepší model\n",
    "model_mlp_drop_l2reg = MLPDropout(input_size=32*32, \n",
    "                                  hidden_layers=networks[index]['hidden_layers'], \n",
    "                                  output_size=networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_mlp_drop_l2reg.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False, False, False,  True,  True,  True, False,\n",
      "        False,  True, False,  True, False,  True, False,  True, False,  True,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False,  True,  True, False, False,  True,  True,\n",
      "        False, False, False,  True], device='cuda:0')\n",
      "tensor([5, 5, 3, 7, 7, 2, 2, 3, 8, 2, 0, 5, 2, 0, 9, 7, 8, 5, 2, 8, 8, 5, 3, 1,\n",
      "        0, 4, 6, 2, 5, 2, 5, 9, 2, 8, 4, 9, 2, 7, 0, 5, 3, 2, 5, 3, 9, 4, 0, 2,\n",
      "        6, 3, 6, 9, 3, 2, 0, 7, 0, 0, 9, 4, 1, 5, 6, 9], device='cuda:0')\n",
      "tensor([9, 5, 3, 7, 7, 2, 2, 3, 8, 8, 0, 5, 4, 0, 9, 7, 8, 5, 4, 8, 8, 5, 3, 1,\n",
      "        6, 4, 6, 2, 5, 4, 7, 9, 8, 8, 3, 9, 4, 7, 6, 5, 3, 6, 5, 3, 9, 4, 0, 2,\n",
      "        6, 3, 6, 9, 3, 6, 0, 7, 6, 3, 9, 4, 1, 7, 0, 9], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_mlp_drop_l2reg(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_mlp_drop_l2reg(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.7369523809523809\n",
      "[[855   6  38  72   5   1  89   0  13   1]\n",
      " [ 20 928  21 103   6   0  14   0   0   0]\n",
      " [  7   2 731   7 136   0 136   0  17   0]\n",
      " [ 98  16  22 750  32   0  89   0   4   3]\n",
      " [  6   0 279  44 513   2 141   0   3   1]\n",
      " [  1   0   1   0   2 842   1 125  17  91]\n",
      " [233   2 282  31 101   2 339   1  38   0]\n",
      " [  0   0   0   0   0  61   0 889   4  88]\n",
      " [  8   1  39  10  10  15  19  15 964   4]\n",
      " [  1   0   2   0   9  43   1  66   4 927]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader.dataset))\n",
    "val_y = np.zeros(len(val_loader.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_mlp_drop_l2reg(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['MLP'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476, 0.837238, 0.836857, 0.819905, 0.802571, 0.736952],\n",
       " 'CNN': []}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bohužel, kombinace regularizací stále zhoršuje přesnost modelů"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(b). SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tedy, zkusím dopřednou neuronovou sít s stochastickým gradientním sestupem  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Namísto optimizeru Adam použiju optimizer SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 6.246662139892578\n",
      "Total training loss: 1.985782526909359\n"
     ]
    }
   ],
   "source": [
    "model = MLP(32*32, [64,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "outputs = model(pixels)\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9) \n",
    "\n",
    "loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_mlp_sgd = {}\n",
    "\n",
    "# Loop through each network configuration\n",
    "for index, network in enumerate(networks):\n",
    "    input_size = 32 * 32  # Input image size\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    hidden_layers = network['hidden_layers']  # Hidden layer sizes\n",
    "    \n",
    "    # Create an instance of MyNet with the specified architecture\n",
    "    model = MLP(input_size, hidden_layers, output_size).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9) \n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):    \n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset)\n",
    "        vacc = vcorrect / len(val_dataset)\n",
    "\n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_mlp_sgd.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_mlp_sgd[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2.303, 1: 0.461, 4: 0.422}\n",
      "{'hidden_layers': [1024, 512, 256, 128], 'output_size': 40}\n",
      "0.422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_mlp_sgd)\n",
    "index = min(best_mlp_sgd, key=best_mlp_sgd.get)\n",
    "print(networks[index])\n",
    "print(best_mlp_sgd[index])\n",
    "\n",
    "model_path = \"saves/best_model_mlp_sgd.pt\"\n",
    "# načteme nejlepší model\n",
    "model_mlp_sgd = MLP(input_size=32*32, \n",
    "                    hidden_layers=networks[index]['hidden_layers'], \n",
    "                    output_size=networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_mlp_sgd.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False,  True,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "tensor([5, 8, 0, 5, 1, 7, 7, 1, 2, 7, 3, 9, 1, 6, 2, 5, 2, 2, 0, 2, 7, 9, 4, 9,\n",
      "        0, 8, 2, 1, 9, 8, 3, 7, 5, 1, 5, 1, 7, 7, 9, 3, 5, 0, 4, 0, 5, 1, 0, 3,\n",
      "        1, 8, 0, 4, 2, 7, 6, 7, 1, 0, 8, 5, 5, 8, 1, 1], device='cuda:0')\n",
      "tensor([5, 8, 0, 5, 1, 7, 7, 1, 2, 7, 3, 9, 1, 6, 4, 5, 2, 2, 0, 2, 7, 9, 4, 7,\n",
      "        0, 8, 2, 1, 9, 8, 3, 7, 5, 1, 5, 1, 7, 7, 9, 3, 5, 0, 4, 3, 5, 1, 0, 0,\n",
      "        1, 8, 0, 4, 2, 7, 6, 7, 1, 0, 8, 5, 5, 8, 1, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_mlp_sgd(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_mlp_sgd(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8563809523809524\n",
      "[[ 965    1   25   40    5    2   34    0    8    0]\n",
      " [   5 1058    5   17    5    0    2    0    0    0]\n",
      " [  14    2  850    7  117    0   36    0   10    0]\n",
      " [  51    8   22  866   50    0   11    0    5    1]\n",
      " [   3    2   96   32  791    0   62    0    3    0]\n",
      " [   1    0    0    0    0  996    1   51    8   23]\n",
      " [ 254    0  143   27  113    0  479    0   13    0]\n",
      " [   0    0    0    0    0   35    0  946    2   59]\n",
      " [   9    0   12    4    8    7    5    5 1035    0]\n",
      " [   0    1    0    0    1   12    0   33    0 1006]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader.dataset))\n",
    "val_y = np.zeros(len(val_loader.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_mlp_sgd(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['MLP'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidím, že optimizer SGD zlepšil přesnost modelu dopředné neuronové síti. Prozatím je to nejlepší model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476, 0.837238, 0.836857, 0.819905, 0.802571, 0.736952, 0.856381],\n",
       " 'CNN': []}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(b). Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusím regularizaci Batch Normalization. Napíšu novou třídu MLPBatch s integrací dávkové normalizace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBatch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, output_size):\n",
    "        super(MLPBatch, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_layers[0])\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_layers[0])\n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers)-1)])\n",
    "        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(size) for size in hidden_layers[1:]])\n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        for layer, bn_layer in zip(self.hidden_layers, self.bn_layers):\n",
    "            x = F.relu(bn_layer(layer(x)))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 2.357325315475464\n",
      "Total training loss: 0.8469422789906699\n"
     ]
    }
   ],
   "source": [
    "model = MLPBatch(32*32, [64,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "outputs = model(pixels)\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_mlp_batch = {}\n",
    "\n",
    "# Loop through each network configuration\n",
    "for index, network in enumerate(networks):\n",
    "    input_size = 32 * 32  # Input image size\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    hidden_layers = network['hidden_layers']  # Hidden layer sizes\n",
    "    \n",
    "    # Create an instance of MyNet with the specified architecture\n",
    "    model = MLPBatch(input_size, hidden_layers, output_size).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):    \n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset)\n",
    "        vacc = vcorrect / len(val_dataset)\n",
    "    \n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_mlp_batch.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_mlp_batch[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.442, 1: 0.419}\n",
      "{'hidden_layers': [128, 64, 32], 'output_size': 10}\n",
      "0.419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_mlp_batch)\n",
    "index = min(best_mlp_batch, key=best_mlp_batch.get)\n",
    "print(networks[index])\n",
    "print(best_mlp_batch[index])\n",
    "\n",
    "model_path = \"saves/best_model_mlp_batch.pt\"\n",
    "# načteme nejlepší model\n",
    "model_mlp_batch = MLPBatch(input_size=32*32, \n",
    "                            hidden_layers=networks[index]['hidden_layers'], \n",
    "                            output_size=networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_mlp_batch.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True,  True, False, False,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True, False,  True,  True,  True,\n",
      "         True,  True, False,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True, False,  True, False,\n",
      "         True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "tensor([1, 8, 9, 9, 8, 4, 5, 7, 1, 1, 4, 8, 9, 7, 8, 2, 3, 5, 6, 6, 4, 2, 4, 3,\n",
      "        6, 7, 6, 1, 1, 2, 3, 5, 6, 7, 1, 7, 0, 9, 5, 0, 9, 5, 3, 0, 6, 0, 2, 0,\n",
      "        5, 5, 3, 2, 3, 1, 1, 9, 4, 8, 2, 5, 7, 1, 2, 8], device='cuda:0')\n",
      "tensor([1, 8, 9, 9, 8, 4, 5, 5, 1, 1, 4, 8, 9, 7, 8, 6, 4, 5, 6, 6, 4, 2, 4, 3,\n",
      "        2, 7, 0, 1, 1, 2, 3, 5, 0, 7, 1, 9, 0, 9, 5, 0, 9, 5, 0, 0, 6, 0, 2, 6,\n",
      "        5, 7, 3, 2, 3, 1, 1, 9, 2, 8, 2, 5, 7, 1, 2, 8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_mlp_batch(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_mlp_batch(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8437142857142857\n",
      "[[ 893    1   28   72    5    3   72    0    6    0]\n",
      " [   5 1055    2   20    3    0    7    0    0    0]\n",
      " [   6    4  745   13  142    0  120    0    6    0]\n",
      " [  41   17   12  876   40    0   25    0    3    0]\n",
      " [   2    3   68   52  788    1   74    0    1    0]\n",
      " [   1    1    0    0    0  965    0   74    7   32]\n",
      " [ 174    7   93   43   89    0  607    0   16    0]\n",
      " [   0    0    0    0    0   47    0  919    3   73]\n",
      " [   6    0   13   18    7    6   10    8 1017    0]\n",
      " [   0    1    1    1    0   15    0   40    1  994]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader.dataset))\n",
    "val_y = np.zeros(len(val_loader.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_mlp_batch(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['MLP'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Přesnost je menší než u modelu s SGD optimizerem, ale je stále dost dobrá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476,\n",
       "  0.837238,\n",
       "  0.836857,\n",
       "  0.819905,\n",
       "  0.802571,\n",
       "  0.736952,\n",
       "  0.856381,\n",
       "  0.843714],\n",
       " 'CNN': []}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II(c). Implementace a použití modelů Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ted použiju model konvoluční neuronové sítě a budu ho ladit pomocí různých technik. Myslím si, že tyto modely budou o více lépe než modely dopředné neuronové síti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(c). Original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusím modely konvoluční neuronové sítě na původních datech a zároven vyladím model na různých hloubkách a velikostech vrstev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Napíšu třídu konvoluční neuronové síti s nastavitelnou hloubkou a velikostmi každou vrstvy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_conv_layers, num_filters, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList([nn.Conv2d(in_channels=1 if i == 0 else num_filters[i - 1],\n",
    "                                                    out_channels=num_filters[i],\n",
    "                                                    kernel_size=3,\n",
    "                                                    stride=1,\n",
    "                                                    padding=1) for i in range(num_conv_layers)])\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate the size of the last convolutional layer's output\n",
    "        # based on the input size (32x32) and the number of conv layers\n",
    "        conv_output_size = 32 // (2 ** num_conv_layers)  # Formula for max pooling\n",
    "        self.fc = nn.Linear(num_filters[-1] * conv_output_size * conv_output_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv_layer in enumerate(self.conv_layers):\n",
    "            x = F.relu(conv_layer(x))\n",
    "            x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dál budu aplikovat té samé procedury trénování modelů neuronových sítí, jako v předchozích dopředných neuronových sítích"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CNN model\n",
    "model = CNN(2, [16,32], 10).to(device)\n",
    "# Now you can pass 'pixels' to the model\n",
    "outputs = model(pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 18.974464416503906\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loss_fn, optimizer, train_loader):\n",
    "    running_cum_loss = 0.0\n",
    "    total_samples = len(train_loader.dataset)\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_cum_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss across all batches\n",
    "    average_loss = running_cum_loss / len(train_loader)\n",
    "\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.0122028169719008\n"
     ]
    }
   ],
   "source": [
    "loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = [\n",
    "    {'num_conv_layers': 2, 'num_filters': [16, 32], 'output_size': 10},  # CNN with 2 conv layers, 16 and 32 filters\n",
    "    {'num_conv_layers': 3, 'num_filters': [32, 64, 128], 'output_size': 10},  # CNN with 3 conv layers, 32, 64, and 128 filters\n",
    "    {'num_conv_layers': 4, 'num_filters': [64, 128, 256, 512], 'output_size': 20},  # CNN with 4 conv layers, 64, 128, 256, and 512 filters\n",
    "    {'num_conv_layers': 5, 'num_filters': [128, 256, 512, 512, 1024], 'output_size': 20},  # CNN with 5 conv layers, 128, 256, 512, 512, and 1024 filters\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_cnn = {}\n",
    "\n",
    "\n",
    "for index, network in enumerate(networks):\n",
    "    num_conv_layers = network['num_conv_layers']  # Number of convolutional layers\n",
    "    num_filters = network['num_filters']  # Number of filters in each convolutional layer\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    \n",
    "    # Create an instance of MyCNN with the specified architecture\n",
    "    model = CNN(num_conv_layers, num_filters, output_size).to(device)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset)\n",
    "        vacc = vcorrect / len(val_dataset)\n",
    "\n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_cnn.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_cnn[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.408, 1: 0.36, 2: 0.338, 3: 0.328}\n",
      "{'num_conv_layers': 5, 'num_filters': [128, 256, 512, 512, 1024], 'output_size': 20}\n",
      "0.328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_cnn)\n",
    "index = min(best_cnn, key=best_cnn.get)\n",
    "print(networks[index])\n",
    "print(best_cnn[index])\n",
    "\n",
    "model_path = \"saves/best_model_cnn.pt\"\n",
    "# načteme nejlepší model\n",
    "model_cnn = CNN(num_conv_layers = networks[index]['num_conv_layers'],\n",
    "                 num_filters = networks[index]['num_filters'], \n",
    "                 output_size = networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_cnn.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8834285714285715\n",
      "[[ 790    0   11   22    6    0  247    0    4    0]\n",
      " [   2 1074    0   14    0    0    2    0    0    0]\n",
      " [  10    3  839    8   91    0   83    0    2    0]\n",
      " [  29    7    5  869   56    0   47    0    1    0]\n",
      " [   1    1   82   21  806    0   76    0    2    0]\n",
      " [   1    0    0    0    0 1044    0   31    0    4]\n",
      " [  76    1   59   20   67    1  798    0    7    0]\n",
      " [   0    0    0    0    0   24    0  987    1   30]\n",
      " [   5    0    4    4    3    2    8    3 1055    1]\n",
      " [   0    0    0    0    0    5    4   30    0 1014]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader.dataset))\n",
    "val_y = np.zeros(len(val_loader.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_cnn(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['CNN'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak jsem si myslel, konvoluční neuronová sít bude o víc lépe predikovat než dopředné. Přesnost konvoluční neuronové síti je o víc vyšší než přesnost nejlepšího modelu dopředné neuronové síti "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476,\n",
       "  0.837238,\n",
       "  0.836857,\n",
       "  0.819905,\n",
       "  0.802571,\n",
       "  0.736952,\n",
       "  0.856381,\n",
       "  0.843714],\n",
       " 'CNN': [0.883429]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(c). Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podívám se jak to bude predikovat se standardními daty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader_stand)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 2.29339861869812\n",
      "Total training loss: 0.7085395553895475\n"
     ]
    }
   ],
   "source": [
    "model = CNN(2, [16,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss = train_one_epoch(model, loss_fn, optimizer, train_loader_stand)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_cnn_stand = {}\n",
    "\n",
    "\n",
    "for index, network in enumerate(networks):\n",
    "    num_conv_layers = network['num_conv_layers']  # Number of convolutional layers\n",
    "    num_filters = network['num_filters']  # Number of filters in each convolutional layer\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    \n",
    "    # Create an instance of MyCNN with the specified architecture\n",
    "    model = CNN(num_conv_layers, num_filters, output_size).to(device)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch(model, loss_fn, optimizer, train_loader_stand)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader_stand):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset_stand)\n",
    "        vacc = vcorrect / len(val_dataset_stand)\n",
    "\n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_cnn_stand.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_cnn_stand[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.371, 1: 0.336, 2: 0.312}\n",
      "{'num_conv_layers': 4, 'num_filters': [64, 128, 256, 512], 'output_size': 20}\n",
      "0.312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_cnn_stand)\n",
    "index = min(best_cnn_stand, key=best_cnn_stand.get)\n",
    "print(networks[index])\n",
    "print(best_cnn_stand[index])\n",
    "\n",
    "model_path = \"saves/best_model_cnn_stand.pt\"\n",
    "# načteme nejlepší model\n",
    "model_cnn_stand = CNN(num_conv_layers = networks[index]['num_conv_layers'],\n",
    "                 num_filters = networks[index]['num_filters'], \n",
    "                 output_size = networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_cnn_stand.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
      "         True,  True,  True,  True,  True,  True, False, False,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "tensor([1, 9, 3, 8, 4, 4, 8, 8, 9, 5, 6, 7, 4, 3, 7, 2, 5, 3, 5, 6, 7, 2, 0, 9,\n",
      "        4, 2, 6, 8, 3, 4, 4, 4, 5, 9, 8, 5, 8, 0, 6, 7, 0, 2, 9, 4, 8, 0, 5, 5,\n",
      "        6, 4, 8, 0, 4, 1, 1, 2, 0, 0, 6, 5, 4, 3, 9, 1], device='cuda:0')\n",
      "tensor([1, 9, 3, 8, 4, 6, 8, 8, 9, 5, 6, 7, 4, 3, 7, 2, 5, 3, 5, 6, 7, 2, 3, 9,\n",
      "        4, 2, 6, 8, 3, 4, 4, 4, 5, 9, 8, 5, 8, 0, 6, 7, 0, 2, 9, 4, 8, 0, 5, 5,\n",
      "        4, 2, 8, 0, 4, 1, 1, 2, 6, 6, 6, 5, 4, 3, 9, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_cnn_stand(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_cnn_stand(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8822857142857143\n",
      "[[ 913    1   13   25    1    2  120    0    5    0]\n",
      " [   2 1068    1   12    1    0    7    0    0    1]\n",
      " [  12    3  871    7   75    0   63    0    5    0]\n",
      " [  37    5    9  887   24    0   46    0    6    0]\n",
      " [   2    2   94   50  746    0   94    0    1    0]\n",
      " [   0    0    0    0    0 1037    0   31    4    8]\n",
      " [ 152    0   77   25   86    0  680    1    8    0]\n",
      " [   0    0    0    0    0   20    0  996    0   26]\n",
      " [   4    0    4    3    1    2    7    3 1061    0]\n",
      " [   0    0    0    0    0   10    0   37    1 1005]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader_stand.dataset))\n",
    "val_y = np.zeros(len(val_loader_stand.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader_stand:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_cnn_stand(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['CNN'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Přesnost však zůstala téměř stejná"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476,\n",
       "  0.837238,\n",
       "  0.836857,\n",
       "  0.819905,\n",
       "  0.802571,\n",
       "  0.736952,\n",
       "  0.856381,\n",
       "  0.843714],\n",
       " 'CNN': [0.883429, 0.882286]}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(c). MinMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusím konvouční neuronovou sít na datech předzpracovaných pomocí MinMax normalizace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader_min_max)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 2.3004589080810547\n",
      "Total training loss: 0.7945007773127566\n"
     ]
    }
   ],
   "source": [
    "model = CNN(2, [16,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss = train_one_epoch(model, loss_fn, optimizer, train_loader_min_max)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_cnn_min_max = {}\n",
    "\n",
    "\n",
    "for index, network in enumerate(networks):\n",
    "    num_conv_layers = network['num_conv_layers']  # Number of convolutional layers\n",
    "    num_filters = network['num_filters']  # Number of filters in each convolutional layer\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    \n",
    "    # Create an instance of MyCNN with the specified architecture\n",
    "    model = CNN(num_conv_layers, num_filters, output_size).to(device)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch(model, loss_fn, optimizer, train_loader_min_max)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader_min_max):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset_min_max)\n",
    "        vacc = vcorrect / len(val_dataset_min_max)\n",
    "\n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_cnn_min_max.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_cnn_min_max[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.355, 1: 0.311, 2: 0.298}\n",
      "{'num_conv_layers': 4, 'num_filters': [64, 128, 256, 512], 'output_size': 20}\n",
      "0.298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_cnn_min_max)\n",
    "index = min(best_cnn_min_max, key=best_cnn_min_max.get)\n",
    "print(networks[index])\n",
    "print(best_cnn_min_max[index])\n",
    "\n",
    "model_path = \"saves/best_model_cnn_min_max.pt\"\n",
    "# načteme nejlepší model\n",
    "model_cnn_min_max = CNN(num_conv_layers = networks[index]['num_conv_layers'],\n",
    "                 num_filters = networks[index]['num_filters'], \n",
    "                 output_size = networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_cnn_min_max.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "tensor([0, 4, 3, 6, 6, 5, 4, 9, 2, 0, 7, 5, 9, 6, 0, 3, 4, 6, 2, 5, 9, 7, 9, 8,\n",
      "        8, 5, 2, 0, 5, 8, 0, 5, 5, 5, 4, 5, 4, 3, 0, 0, 2, 6, 5, 4, 2, 7, 0, 4,\n",
      "        4, 0, 3, 7, 2, 1, 0, 4, 1, 5, 3, 9, 1, 0, 7, 1], device='cuda:0')\n",
      "tensor([0, 4, 3, 6, 6, 5, 4, 9, 2, 0, 7, 5, 9, 6, 0, 3, 4, 6, 2, 5, 9, 7, 9, 8,\n",
      "        8, 5, 2, 0, 5, 8, 6, 5, 5, 5, 4, 5, 4, 3, 6, 0, 4, 6, 5, 4, 2, 7, 0, 4,\n",
      "        4, 0, 3, 7, 2, 1, 0, 4, 1, 5, 3, 9, 1, 0, 7, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_cnn_min_max(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_cnn_min_max(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8935238095238095\n",
      "[[ 894    5   12   51    3    1  111    0    3    0]\n",
      " [   1 1083    0    5    3    0    0    0    0    0]\n",
      " [  14    3  844   10   97    0   64    0    4    0]\n",
      " [  21   12    5  932   21    0   22    0    1    0]\n",
      " [   0    0   61   48  793    0   85    0    2    0]\n",
      " [   0    0    0    0    0 1042    0   32    0    6]\n",
      " [ 130    3   60   36   70    0  725    0    5    0]\n",
      " [   0    0    0    0    0   18    0  997    0   27]\n",
      " [   6    0    2    4    1    3    9    2 1058    0]\n",
      " [   0    2    0    1    0    5    0   31    0 1014]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader_min_max.dataset))\n",
    "val_y = np.zeros(len(val_loader_min_max.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader_min_max:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_cnn_min_max(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['CNN'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476,\n",
       "  0.837238,\n",
       "  0.836857,\n",
       "  0.819905,\n",
       "  0.802571,\n",
       "  0.736952,\n",
       "  0.856381,\n",
       "  0.843714],\n",
       " 'CNN': [0.883429, 0.882286, 0.893524]}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K mému překvapení jsem se dostal vyšší přesnost. Ale přesto zkusím ještě jiné optimalizační a regularizační metody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(c). Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusím Dropout regularizaci. Trochu změním třídu pro implementaci Dropout atributů. Trénováné ale bude probíhat stejně"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDropout(nn.Module):\n",
    "    def __init__(self, num_conv_layers, num_filters, output_size, dropout_p=0.1):\n",
    "        super(CNNDropout, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList([nn.Sequential(\n",
    "                                            nn.Conv2d(in_channels=1 if i == 0 else num_filters[i - 1],\n",
    "                                                      out_channels=num_filters[i],\n",
    "                                                      kernel_size=3,\n",
    "                                                      stride=1,\n",
    "                                                      padding=1),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                                            nn.Dropout2d(dropout_p)) for i in range(num_conv_layers)])\n",
    "        \n",
    "        # Calculate the size of the last convolutional layer's output\n",
    "        # based on the input size (32x32) and the number of conv layers\n",
    "        conv_output_size = 32 // (2 ** num_conv_layers)  # Formula for max pooling\n",
    "        self.fc = nn.Linear(num_filters[-1] * conv_output_size * conv_output_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = conv_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 16.000905990600586\n",
      "Total training loss: 0.9195673057684792\n"
     ]
    }
   ],
   "source": [
    "model = CNNDropout(2, [16,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_cnn_drop = {}\n",
    "\n",
    "\n",
    "for index, network in enumerate(networks):\n",
    "    num_conv_layers = network['num_conv_layers']  # Number of convolutional layers\n",
    "    num_filters = network['num_filters']  # Number of filters in each convolutional layer\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    \n",
    "    # Create an instance of MyCNN with the specified architecture\n",
    "    model = CNNDropout(num_conv_layers, num_filters, output_size).to(device)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset)\n",
    "        vacc = vcorrect / len(val_dataset)\n",
    "\n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_cnn_drop.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_cnn_drop[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.37, 1: 0.338, 2: 0.316}\n",
      "{'num_conv_layers': 4, 'num_filters': [64, 128, 256, 512], 'output_size': 20}\n",
      "0.316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_cnn_drop)\n",
    "index = min(best_cnn_drop, key=best_cnn_drop.get)\n",
    "print(networks[index])\n",
    "print(best_cnn_drop[index])\n",
    "\n",
    "model_path = \"saves/best_model_cnn_drop.pt\"\n",
    "# načteme nejlepší model\n",
    "model_cnn_drop = CNNDropout(num_conv_layers = networks[index]['num_conv_layers'],\n",
    "                 num_filters = networks[index]['num_filters'], \n",
    "                 output_size = networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_cnn_drop.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True, False,  True, False,\n",
      "         True, False,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "tensor([8, 4, 4, 4, 1, 4, 9, 7, 2, 7, 0, 4, 1, 1, 2, 7, 7, 7, 8, 9, 7, 8, 8, 0,\n",
      "        1, 4, 1, 5, 0, 8, 2, 7, 6, 5, 1, 1, 4, 4, 1, 5, 9, 1, 4, 0, 9, 4, 9, 6,\n",
      "        3, 4, 3, 7, 0, 3, 8, 0, 2, 1, 3, 0, 0, 0, 0, 7], device='cuda:0')\n",
      "tensor([8, 4, 4, 4, 1, 4, 9, 7, 2, 7, 0, 4, 1, 1, 2, 7, 7, 7, 8, 9, 7, 8, 8, 0,\n",
      "        1, 4, 1, 5, 0, 8, 4, 7, 6, 5, 1, 1, 4, 4, 1, 5, 9, 1, 4, 0, 9, 4, 9, 6,\n",
      "        3, 3, 3, 7, 0, 3, 8, 0, 2, 1, 3, 3, 6, 0, 6, 7], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_cnn_drop(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_cnn_drop(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8737142857142857\n",
      "[[ 921    0   19   37    1    0   90    0   12    0]\n",
      " [   1 1070    0   17    1    0    0    0    2    1]\n",
      " [  13    3  839   10  102    0   63    0    6    0]\n",
      " [  42    5   17  899   27    0   22    0    2    0]\n",
      " [   3    3  100   55  755    0   70    0    3    0]\n",
      " [   0    0    0    0    0 1004    0   48    6   22]\n",
      " [ 178    1   77   39  103    0  622    0    9    0]\n",
      " [   0    0    0    0    0   16    0  994    1   31]\n",
      " [   7    0    4    3    0    3    3    2 1063    0]\n",
      " [   0    1    0    3    0    2    0   37    3 1007]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader.dataset))\n",
    "val_y = np.zeros(len(val_loader.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_cnn_drop(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['CNN'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476,\n",
       "  0.837238,\n",
       "  0.836857,\n",
       "  0.819905,\n",
       "  0.802571,\n",
       "  0.736952,\n",
       "  0.856381,\n",
       "  0.843714],\n",
       " 'CNN': [0.883429, 0.882286, 0.893524, 0.873714]}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dostal jsem nižší přesnost při využití dropout regularizaci, ale ona je stále vysoká"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(c). Dropout + L1reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusím kombinaci Dropout a L1 regularizaci. Podívám se, co z toho vyjde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 20.729663848876953\n"
     ]
    }
   ],
   "source": [
    "model = CNNDropout(2, [16,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako u dopředné neuronové síti změním trénovácí funkci s využitím L1 regularizaci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_l1reg(model, loss_fn, optimizer, train_loader, l1_lambda=0.1):\n",
    "    running_cum_loss = 0.0\n",
    "    total_samples = len(train_loader.dataset)\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        l1_reg = torch.tensor(0., device='cuda:0')\n",
    "        l1_reg += torch.linalg.vector_norm(model.fc.weight.flatten(),1)\n",
    "\n",
    "        loss = loss + l1_lambda*l1_reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_cum_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss across all batches\n",
    "    average_loss = running_cum_loss / len(train_loader)\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 4.14905073724944\n"
     ]
    }
   ],
   "source": [
    "loss = train_one_epoch_l1reg(model, loss_fn, optimizer, train_loader)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_cnn_drop_l1reg = {}\n",
    "\n",
    "\n",
    "for index, network in enumerate(networks):\n",
    "    num_conv_layers = network['num_conv_layers']  # Number of convolutional layers\n",
    "    num_filters = network['num_filters']  # Number of filters in each convolutional layer\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    \n",
    "    # Create an instance of MyCNN with the specified architecture\n",
    "    model = CNNDropout(num_conv_layers, num_filters, output_size).to(device)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch_l1reg(model, loss_fn, optimizer, train_loader)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset)\n",
    "        vacc = vcorrect / len(val_dataset)\n",
    "\n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_cnn_drop_l1reg.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_cnn_drop_l1reg[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.569, 1: 0.396, 2: 0.338}\n",
      "{'num_conv_layers': 4, 'num_filters': [64, 128, 256, 512], 'output_size': 20}\n",
      "0.338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_cnn_drop_l1reg)\n",
    "index = min(best_cnn_drop_l1reg, key=best_cnn_drop_l1reg.get)\n",
    "print(networks[index])\n",
    "print(best_cnn_drop_l1reg[index])\n",
    "\n",
    "model_path = \"saves/best_model_cnn_drop_l1reg.pt\"\n",
    "# načteme nejlepší model\n",
    "model_cnn_drop_l1reg = CNNDropout(num_conv_layers = networks[index]['num_conv_layers'],\n",
    "                 num_filters = networks[index]['num_filters'], \n",
    "                 output_size = networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_cnn_drop_l1reg.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True,  True,  True,  True, False, False,  True,\n",
      "        False,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True, False,  True,  True], device='cuda:0')\n",
      "tensor([4, 2, 7, 4, 0, 3, 2, 3, 2, 7, 7, 6, 9, 8, 8, 2, 7, 3, 8, 8, 1, 4, 1, 5,\n",
      "        2, 8, 4, 6, 5, 7, 8, 4, 8, 3, 2, 2, 5, 0, 8, 9, 1, 0, 6, 0, 0, 4, 9, 1,\n",
      "        9, 5, 9, 0, 5, 9, 3, 0, 1, 2, 8, 6, 4, 6, 2, 8], device='cuda:0')\n",
      "tensor([4, 2, 7, 4, 0, 3, 2, 0, 4, 7, 5, 6, 9, 8, 8, 2, 7, 3, 8, 8, 1, 4, 1, 5,\n",
      "        2, 8, 4, 6, 5, 7, 8, 4, 8, 3, 2, 2, 5, 0, 8, 9, 1, 0, 6, 0, 0, 4, 9, 1,\n",
      "        9, 5, 9, 0, 5, 9, 3, 0, 1, 2, 8, 6, 4, 0, 2, 8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_cnn_drop_l1reg(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_cnn_drop_l1reg(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8730476190476191\n",
      "[[ 894    4   12   62    1    2  101    0    4    0]\n",
      " [   1 1079    0   10    0    0    2    0    0    0]\n",
      " [  23    4  798   12  102    0   86    1   10    0]\n",
      " [  23   16    8  923   19    0   22    0    3    0]\n",
      " [   5    5   74   64  738    0   94    0    9    0]\n",
      " [   1    0    0    0    0 1044    0   28    3    4]\n",
      " [ 154    8   64   43   69    1  682    1    7    0]\n",
      " [   0    0    0    0    0   46    0  987    1    8]\n",
      " [   2    1    2    8    1    9    9    3 1047    3]\n",
      " [   0    1    0    0    0   12    0   65    0  975]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader.dataset))\n",
    "val_y = np.zeros(len(val_loader.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_cnn_drop_l1reg(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['CNN'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Přesnost se trochu zmenšila, tedy zkusím jiné metody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476,\n",
       "  0.837238,\n",
       "  0.836857,\n",
       "  0.819905,\n",
       "  0.802571,\n",
       "  0.736952,\n",
       "  0.856381,\n",
       "  0.843714],\n",
       " 'CNN': [0.883429, 0.882286, 0.893524, 0.873714, 0.873048]}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(c). Dropout + L2reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zkusím ted kombinaci Dropout + L2 regularizaci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 15.132769584655762\n"
     ]
    }
   ],
   "source": [
    "model = CNNDropout(2, [16,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Přepísu trochu trénovácí funkci s vuyžitím L2 regularizaci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_l2reg(model, loss_fn, optimizer, train_loader, l2_lambda=0.1):\n",
    "    running_cum_loss = 0.0\n",
    "    total_samples = len(train_loader.dataset)\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Calculate L2 regularization term\n",
    "        l2_reg = torch.tensor(0., device='cuda:0')\n",
    "        for param in model.parameters():\n",
    "            l2_reg += torch.norm(param, 2)**2\n",
    "        \n",
    "        # Add L2 regularization term to the loss\n",
    "        loss += 0.5 * l2_lambda * l2_reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_cum_loss += loss.item()\n",
    "\n",
    "    # Calculate the average loss across all batches\n",
    "    average_loss = running_cum_loss / len(train_loader)\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 1.4635184818300708\n"
     ]
    }
   ],
   "source": [
    "loss = train_one_epoch_l2reg(model, loss_fn, optimizer, train_loader)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_cnn_drop_l2reg = {}\n",
    "\n",
    "\n",
    "for index, network in enumerate(networks):\n",
    "    num_conv_layers = network['num_conv_layers']  # Number of convolutional layers\n",
    "    num_filters = network['num_filters']  # Number of filters in each convolutional layer\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    \n",
    "    # Create an instance of MyCNN with the specified architecture\n",
    "    model = CNNDropout(num_conv_layers, num_filters, output_size).to(device)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch_l2reg(model, loss_fn, optimizer, train_loader)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset)\n",
    "        vacc = vcorrect / len(val_dataset)\n",
    "\n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_cnn_drop_l2reg.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_cnn_drop_l2reg[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.423}\n",
      "{'num_conv_layers': 2, 'num_filters': [16, 32], 'output_size': 10}\n",
      "0.423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_cnn_drop_l2reg)\n",
    "index = min(best_cnn_drop_l2reg, key=best_cnn_drop_l2reg.get)\n",
    "print(networks[index])\n",
    "print(best_cnn_drop_l2reg[index])\n",
    "\n",
    "model_path = \"saves/best_model_cnn_drop_l2reg.pt\"\n",
    "# načteme nejlepší model\n",
    "model_cnn_drop_l2reg = CNNDropout(num_conv_layers = networks[index]['num_conv_layers'],\n",
    "                 num_filters = networks[index]['num_filters'], \n",
    "                 output_size = networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_cnn_drop_l2reg.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True, False,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "        False,  True, False,  True, False,  True,  True,  True, False,  True,\n",
      "        False, False,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "tensor([1, 9, 9, 0, 4, 6, 8, 5, 4, 8, 7, 5, 2, 6, 2, 2, 4, 7, 0, 9, 3, 3, 0, 5,\n",
      "        7, 0, 0, 2, 6, 2, 0, 8, 6, 4, 9, 4, 6, 7, 5, 4, 3, 1, 3, 8, 9, 7, 7, 5,\n",
      "        5, 2, 7, 7, 3, 4, 4, 5, 1, 0, 5, 0, 5, 6, 7, 7], device='cuda:0')\n",
      "tensor([1, 9, 9, 6, 4, 2, 8, 5, 4, 8, 7, 5, 2, 6, 4, 6, 4, 7, 0, 9, 4, 3, 6, 5,\n",
      "        5, 0, 0, 2, 0, 2, 6, 4, 6, 4, 9, 2, 6, 7, 5, 6, 3, 1, 0, 8, 9, 7, 7, 5,\n",
      "        5, 2, 9, 7, 3, 4, 4, 5, 1, 0, 5, 0, 5, 6, 7, 7], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_cnn_drop_l2reg(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_cnn_drop_l2reg(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8455238095238096\n",
      "[[ 907    2   13   72    9    4   64    0    9    0]\n",
      " [   0 1055    2   25    4    0    5    0    1    0]\n",
      " [  18    1  813   16  121    0   40    0   27    0]\n",
      " [  38   12   13  885   47    0   13    0    4    2]\n",
      " [   3    1   85   49  767    0   79    0    5    0]\n",
      " [   0    0    0    1    0 1031    0   32    9    7]\n",
      " [ 226    3  100   46  119    2  506    0   27    0]\n",
      " [   0    0    0    0    0   77    0  909    6   50]\n",
      " [  10    1    3   11    6   11    4    2 1036    1]\n",
      " [   1    0    1    3    3   23    0   37   16  969]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader.dataset))\n",
    "val_y = np.zeros(len(val_loader.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_cnn_drop_l2reg(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['CNN'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bohužel, dostal jsem přesnost jako u dopředných neuronových sítí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476,\n",
       "  0.837238,\n",
       "  0.836857,\n",
       "  0.819905,\n",
       "  0.802571,\n",
       "  0.736952,\n",
       "  0.856381,\n",
       "  0.843714],\n",
       " 'CNN': [0.883429, 0.882286, 0.893524, 0.873714, 0.873048, 0.845524]}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(c). SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podívám se, jak SGD optimizer ovlivnuje modely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 32.55213928222656\n",
      "Total training loss: 1.092496575164263\n"
     ]
    }
   ],
   "source": [
    "model = CNN(2, [16,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9) \n",
    "\n",
    "loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_cnn_sgd = {}\n",
    "\n",
    "\n",
    "for index, network in enumerate(networks):\n",
    "    num_conv_layers = network['num_conv_layers']  # Number of convolutional layers\n",
    "    num_filters = network['num_filters']  # Number of filters in each convolutional layer\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    \n",
    "    # Create an instance of MyCNN with the specified architecture\n",
    "    model = CNN(num_conv_layers, num_filters, output_size).to(device)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9) \n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset)\n",
    "        vacc = vcorrect / len(val_dataset)\n",
    "\n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_cnn_sgd.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_cnn_sgd[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.401, 1: 0.35, 3: 0.308}\n",
      "{'num_conv_layers': 5, 'num_filters': [128, 256, 512, 512, 1024], 'output_size': 20}\n",
      "0.308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_cnn_sgd)\n",
    "index = min(best_cnn_sgd, key=best_cnn_sgd.get)\n",
    "print(networks[index])\n",
    "print(best_cnn_sgd[index])\n",
    "\n",
    "model_path = \"saves/best_model_cnn_sgd.pt\"\n",
    "# načteme nejlepší model\n",
    "model_cnn_sgd = CNN(num_conv_layers = networks[index]['num_conv_layers'],\n",
    "                 num_filters = networks[index]['num_filters'], \n",
    "                 output_size = networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_cnn_sgd.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  True, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False, False,  True,  True, False,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True,  True,  True, False, False,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True], device='cuda:0')\n",
      "tensor([1, 6, 3, 4, 6, 4, 3, 4, 2, 1, 0, 7, 5, 0, 0, 7, 8, 2, 4, 2, 8, 9, 7, 5,\n",
      "        4, 9, 9, 3, 3, 7, 2, 2, 4, 0, 0, 4, 8, 5, 4, 4, 6, 5, 0, 9, 7, 5, 0, 9,\n",
      "        9, 3, 9, 4, 5, 3, 0, 4, 2, 7, 3, 7, 0, 9, 0, 0], device='cuda:0')\n",
      "tensor([1, 6, 3, 4, 4, 4, 3, 4, 2, 1, 0, 7, 5, 6, 6, 7, 8, 4, 4, 2, 8, 9, 7, 5,\n",
      "        4, 9, 9, 3, 3, 7, 2, 2, 4, 0, 6, 4, 8, 5, 6, 2, 6, 5, 0, 9, 7, 7, 0, 9,\n",
      "        9, 3, 9, 4, 5, 3, 0, 4, 2, 7, 3, 7, 0, 9, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_cnn_sgd(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_cnn_sgd(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8889523809523809\n",
      "[[ 940    0    8   51    2    0   76    0    3    0]\n",
      " [   4 1070    2   15    1    0    0    0    0    0]\n",
      " [  13    2  807   13  114    1   83    0    3    0]\n",
      " [  18    6    5  954   16    0   13    0    2    0]\n",
      " [   0    1   42   73  788    0   83    0    2    0]\n",
      " [   0    0    0    1    0 1060    0   17    0    2]\n",
      " [ 160    1   45   45   62    0  711    0    5    0]\n",
      " [   0    0    0    0    0   76    0  935    1   30]\n",
      " [   5    0    3   10    2    4    4    1 1056    0]\n",
      " [   0    1    0    1    0    9    0   29    0 1013]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader.dataset))\n",
    "val_y = np.zeros(len(val_loader.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_cnn_sgd(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['CNN'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476,\n",
       "  0.837238,\n",
       "  0.836857,\n",
       "  0.819905,\n",
       "  0.802571,\n",
       "  0.736952,\n",
       "  0.856381,\n",
       "  0.843714],\n",
       " 'CNN': [0.883429, 0.882286, 0.893524, 0.873714, 0.873048, 0.845524, 0.888952]}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD optimizer má dobrý vliv na model konvoluční neuronové síti. Prozatím jsem se dostal nejvyšší přesnost predikcí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II(c). Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ted chtěl bych si zkoušet model konvoluční neuronové síti s Batch normalizací a podívat se na jeho vliv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBatch(nn.Module):\n",
    "    def __init__(self, num_conv_layers, num_filters, output_size):\n",
    "        super(CNNBatch, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList([nn.Conv2d(in_channels=1 if i == 0 else num_filters[i - 1],\n",
    "                                                    out_channels=num_filters[i],\n",
    "                                                    kernel_size=3,\n",
    "                                                    stride=1,\n",
    "                                                    padding=1) for i in range(num_conv_layers)])\n",
    "        self.bn_layers = nn.ModuleList([nn.BatchNorm2d(num_filters[i]) for i in range(num_conv_layers)])\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate the size of the last convolutional layer's output\n",
    "        # based on the input size (32x32) and the number of conv layers\n",
    "        conv_output_size = 32 // (2 ** num_conv_layers)  # Formula for max pooling\n",
    "        self.fc = nn.Linear(num_filters[-1] * conv_output_size * conv_output_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (conv_layer, bn_layer) in enumerate(zip(self.conv_layers, self.bn_layers)):\n",
    "            x = conv_layer(x)\n",
    "            x = bn_layer(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "pixels, labels = next(dataiter)\n",
    "batch = next(dataiter)\n",
    "\n",
    "print(pixels.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss for this batch: 2.631425619125366\n",
      "Total training loss: 0.5834839594291625\n"
     ]
    }
   ],
   "source": [
    "model = CNNBatch(2, [16,32], 10).to(device)\n",
    "outputs = model(pixels)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(outputs, labels)\n",
    "print('Total loss for this batch: {}'.format(loss.item()))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "print(f\"Total training loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the best validation loss\n",
    "best_vloss = float('inf')  # Positive infinity to ensure the first validation loss is always better\n",
    "best_cnn_batch = {}\n",
    "\n",
    "\n",
    "for index, network in enumerate(networks):\n",
    "    num_conv_layers = network['num_conv_layers']  # Number of convolutional layers\n",
    "    num_filters = network['num_filters']  # Number of filters in each convolutional layer\n",
    "    output_size = network['output_size']  # Output size for classification\n",
    "    \n",
    "    # Create an instance of MyCNN with the specified architecture\n",
    "    model = CNNBatch(num_conv_layers, num_filters, output_size).to(device)\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    MAX_EPOCHS = 15\n",
    "    K_EPOCHS = 5\n",
    "    \n",
    "    epochs = []\n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    epochs_from_best = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train(True)\n",
    "    \n",
    "        # One training step\n",
    "        avg_loss = train_one_epoch(model, loss_fn, optimizer, train_loader)\n",
    "\n",
    "        model.train(False)\n",
    "    \n",
    "        # Validation performance\n",
    "        running_cum_vloss = 0.0\n",
    "        vcorrect = 0\n",
    "        for i, vdata in enumerate(val_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            with torch.no_grad():\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "            running_cum_vloss += vloss * vinputs.shape[0]\n",
    "            # count the correctly classified samples\n",
    "            vcorrect += (voutputs.argmax(1) == vlabels).float().sum()\n",
    "        # Get average loss and accuracy\n",
    "        avg_vloss = running_cum_vloss / len(val_dataset)\n",
    "        vacc = vcorrect / len(val_dataset)\n",
    "\n",
    "        # Save the model if the validation loss improves\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f\"saves/best_model_cnn_batch.pt\"  # Save models with different output sizes separately\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            best_cnn_batch[index] = round(best_vloss.item(), 3)\n",
    "            epochs_from_best = 0\n",
    "        else:\n",
    "            epochs_from_best += 1\n",
    "            \n",
    "        # EARLY STOPPING\n",
    "        if epochs_from_best > K_EPOCHS:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.35, 1: 0.326, 2: 0.306}\n",
      "{'num_conv_layers': 4, 'num_filters': [64, 128, 256, 512], 'output_size': 20}\n",
      "0.306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_cnn_batch)\n",
    "index = min(best_cnn_batch, key=best_cnn_batch.get)\n",
    "print(networks[index])\n",
    "print(best_cnn_batch[index])\n",
    "\n",
    "model_path = \"saves/best_model_cnn_batch.pt\"\n",
    "# načteme nejlepší model\n",
    "model_cnn_batch = CNNBatch(num_conv_layers = networks[index]['num_conv_layers'],\n",
    "                 num_filters = networks[index]['num_filters'], \n",
    "                 output_size = networks[index]['output_size']).to(device)\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "model_cnn_batch.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True, False,  True,  True,  True,  True, False,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "        False,  True,  True, False], device='cuda:0')\n",
      "tensor([5, 0, 9, 9, 7, 0, 0, 0, 1, 9, 1, 4, 4, 3, 3, 7, 6, 6, 8, 8, 9, 4, 9, 1,\n",
      "        4, 5, 9, 5, 4, 4, 7, 0, 1, 0, 9, 0, 2, 6, 0, 0, 6, 2, 7, 6, 4, 4, 5, 5,\n",
      "        4, 4, 9, 1, 6, 5, 9, 4, 4, 8, 2, 2, 0, 8, 7, 4], device='cuda:0')\n",
      "tensor([5, 6, 7, 9, 7, 0, 0, 0, 1, 9, 1, 4, 4, 3, 3, 7, 6, 6, 8, 8, 9, 4, 9, 1,\n",
      "        4, 5, 9, 5, 4, 4, 7, 0, 1, 2, 9, 0, 2, 6, 0, 0, 6, 2, 7, 6, 2, 4, 5, 5,\n",
      "        4, 3, 9, 1, 6, 5, 9, 4, 4, 8, 2, 2, 6, 8, 7, 6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_cnn_batch(pixels).argmax(1) == labels.flatten())\n",
    "    print(model_cnn_batch(pixels).argmax(1))\n",
    "\n",
    "print(labels.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validacni přesnost: 0.8988571428571429\n",
      "[[ 953    0   20   20    6    0   80    0    1    0]\n",
      " [   1 1075    0   10    3    0    2    0    0    1]\n",
      " [   9    1  859    3  106    1   55    0    2    0]\n",
      " [  36    5   12  881   54    0   26    0    0    0]\n",
      " [   2    0   40   20  881    1   45    0    0    0]\n",
      " [   0    0    0    0    0 1044    0   29    0    7]\n",
      " [ 138    0   61   20  101    0  705    0    4    0]\n",
      " [   0    0    0    0    0   16    0  959    0   67]\n",
      " [   6    0    5    4    2    8   10    0 1049    1]\n",
      " [   0    0    0    0    0    7    0   14    0 1032]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "val_predictions = np.zeros(len(val_loader.dataset))\n",
    "val_y = np.zeros(len(val_loader.dataset))\n",
    "ii = 0\n",
    "for vdata in val_loader:\n",
    "    vinputs, vlabels = vdata\n",
    "    with torch.no_grad():\n",
    "        voutputs = model_cnn_batch(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "    val_predictions[ii:(ii + vinputs.shape[0])] = voutputs.argmax(1).cpu().numpy()\n",
    "    val_y[ii:(ii + vinputs.shape[0])] = vlabels.cpu().numpy()\n",
    "\n",
    "    ii += vinputs.shape[0]\n",
    "    \n",
    "print(f\"Validacni přesnost: {accuracy_score(val_y, val_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(val_y, val_predictions)\n",
    "print(cnf_matrix)\n",
    "\n",
    "accuracy['CNN'].append(round(accuracy_score(val_y, val_predictions),6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak se mi zdá, je to nejvyšší dosažená přesnost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MLP': [0.840476,\n",
       "  0.837238,\n",
       "  0.836857,\n",
       "  0.819905,\n",
       "  0.802571,\n",
       "  0.736952,\n",
       "  0.856381,\n",
       "  0.843714],\n",
       " 'CNN': [0.883429,\n",
       "  0.882286,\n",
       "  0.893524,\n",
       "  0.873714,\n",
       "  0.873048,\n",
       "  0.845524,\n",
       "  0.888952,\n",
       "  0.898857]}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Finální model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ted už můzu vybrat finální model na základech všech zkoušených modelů neuronových sítí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pro snadné zjištění nejlepšího modelu vytvořím dataset přesnosté všech modelů se všemi použitými metodami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               MLP       CNN\n",
      "Original  0.840476  0.883429\n",
      "Standard  0.837238  0.882286\n",
      "MinMax    0.836857  0.893524\n",
      "Dropout   0.819905  0.873714\n",
      "Drop+L1   0.802571  0.873048\n",
      "Drop+L2   0.736952  0.845524\n",
      "SGD       0.856381  0.888952\n",
      "Batch     0.843714  0.898857\n"
     ]
    }
   ],
   "source": [
    "rows_name = ['Original', 'Standard', 'MinMax', 'Dropout', 'Drop+L1', 'Drop+L2', 'SGD', 'Batch']\n",
    "accuracy_df = pd.DataFrame(accuracy, rows_name)\n",
    "\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Accuracy Score: 0.898857\n",
      "Final model:  CNN\n",
      "Method of model tuning:  Batch\n"
     ]
    }
   ],
   "source": [
    "highest_accuracy_score = accuracy_df.max(axis=0).max()\n",
    "model_name = accuracy_df.max().idxmax()\n",
    "row_name = accuracy_df[model_name].idxmax()\n",
    "\n",
    "print(\"Highest Accuracy Score:\", highest_accuracy_score)\n",
    "print(\"Final model: \", model_name)\n",
    "print(\"Method of model tuning: \",row_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidím, že nejvyšší přesnost dosahl model konvoluční neuronové sítí s Batch normalizaci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proto jako finální model zvolím tento model uložený jako best_model_cnn_batch.pt Tento model má 4 vrstvy a následující velikosti vrstev:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_conv_layers': 4, 'num_filters': [64, 128, 256, 512], 'output_size': 20}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = min(best_cnn_batch, key=best_cnn_batch.get)\n",
    "print(networks[index])\n",
    "\n",
    "final_model = model_cnn_batch\n",
    "final_model = final_model.to(device)\n",
    "model_path = \"saves/best_model_cnn_batch.pt\"\n",
    "    \n",
    "# Load the model's state dictionary\n",
    "final_model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podívám se na parametry finálního modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name: conv_layers.0.weight - size of the layer: 576\n",
      "Layer name: conv_layers.0.bias - size of the layer: 64\n",
      "Layer name: conv_layers.1.weight - size of the layer: 73728\n",
      "Layer name: conv_layers.1.bias - size of the layer: 128\n",
      "Layer name: conv_layers.2.weight - size of the layer: 294912\n",
      "Layer name: conv_layers.2.bias - size of the layer: 256\n",
      "Layer name: conv_layers.3.weight - size of the layer: 1179648\n",
      "Layer name: conv_layers.3.bias - size of the layer: 512\n",
      "Layer name: bn_layers.0.weight - size of the layer: 64\n",
      "Layer name: bn_layers.0.bias - size of the layer: 64\n",
      "Layer name: bn_layers.1.weight - size of the layer: 128\n",
      "Layer name: bn_layers.1.bias - size of the layer: 128\n",
      "Layer name: bn_layers.2.weight - size of the layer: 256\n",
      "Layer name: bn_layers.2.bias - size of the layer: 256\n",
      "Layer name: bn_layers.3.weight - size of the layer: 512\n",
      "Layer name: bn_layers.3.bias - size of the layer: 512\n",
      "Layer name: fc.weight - size of the layer: 40960\n",
      "Layer name: fc.bias - size of the layer: 20\n",
      "\n",
      "Celkový počet parametrů: 1592724\n"
     ]
    }
   ],
   "source": [
    "num_params = 0\n",
    "for name, param in final_model.named_parameters():\n",
    "    print(f\"Layer name: {name} - size of the layer: {param.numel()}\")\n",
    "    num_params += param.numel()\n",
    "    \n",
    "print(f\"\\nCelkový počet parametrů: {num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Můj finální model mé celkem 1.6 milionů parametrů"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podívám se na přesnost finálního modelu na testovaích datech, která doposud neviděl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test přesnost: 0.8968571428571429\n",
      "[[ 920    0   19   17    4    0   87    0    0    0]\n",
      " [   0 1008    0    9    5    2    2    0    1    0]\n",
      " [  11    2  871    7  113    0   43    0    0    0]\n",
      " [  27    9   11  940   58    0   25    0    0    2]\n",
      " [   0    0   44   17  959    0   44    0    0    0]\n",
      " [   0    0    0    0    0  962    0   26    0   16]\n",
      " [ 139    2   74   19  113    1  737    0    3    0]\n",
      " [   0    0    0    0    0   15    0  962    0   59]\n",
      " [   4    2    3    3    8    4    8    0 1043    2]\n",
      " [   0    0    0    0    0   10    0   13    0 1015]]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = np.zeros(len(test_loader.dataset))\n",
    "test_y = np.zeros(len(test_loader.dataset))\n",
    "ii = 0\n",
    "for tdata in test_loader:\n",
    "    tinputs, tlabels = tdata\n",
    "    with torch.no_grad():\n",
    "        toutputs = final_model(tinputs)\n",
    "        tloss = loss_fn(toutputs, tlabels)\n",
    "    test_predictions[ii:(ii + tinputs.shape[0])] = toutputs.argmax(1).cpu().numpy()\n",
    "    test_y[ii:(ii + tinputs.shape[0])] = tlabels.cpu().numpy()\n",
    "\n",
    "    ii += tinputs.shape[0]\n",
    "    \n",
    "print(f\"Test přesnost: {accuracy_score(test_y, test_predictions)}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(test_y, test_predictions)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dostal jsem na testovacích datech dost vysokou přesnost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "sns.heatmap(cnf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "\n",
    "plt.gca().spines['top'].set_visible(True)\n",
    "plt.gca().spines['bottom'].set_visible(True)\n",
    "plt.gca().spines['left'].set_visible(True)\n",
    "plt.gca().spines['right'].set_visible(True)\n",
    "\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ale všiml jsem si, že téměř všechny modely má nějaké problémy s predikcemi košil. Všechny si pletou košile s kabáty a tričky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Predikce pomocí finálního modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uložím data na vyhodnocování do dataframu, aplikuju všechny manipulace pro nej, abych mohl správně použít můj finální model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>pix1</th>\n",
       "      <th>pix2</th>\n",
       "      <th>pix3</th>\n",
       "      <th>pix4</th>\n",
       "      <th>pix5</th>\n",
       "      <th>pix6</th>\n",
       "      <th>pix7</th>\n",
       "      <th>pix8</th>\n",
       "      <th>pix9</th>\n",
       "      <th>...</th>\n",
       "      <th>pix1015</th>\n",
       "      <th>pix1016</th>\n",
       "      <th>pix1017</th>\n",
       "      <th>pix1018</th>\n",
       "      <th>pix1019</th>\n",
       "      <th>pix1020</th>\n",
       "      <th>pix1021</th>\n",
       "      <th>pix1022</th>\n",
       "      <th>pix1023</th>\n",
       "      <th>pix1024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  pix1  pix2  pix3  pix4  pix5  pix6  pix7  pix8  pix9  ...  pix1015  \\\n",
       "0   0     0     0     0     0     0     0     0     0     0  ...        0   \n",
       "1   1     0     0     0     0     0     0     0     0     0  ...        0   \n",
       "2   2     0     0     0     0     0     0     0     0     0  ...        0   \n",
       "3   3     2     2     2     2     2     2     2     2     2  ...        2   \n",
       "4   4     0     0     0     0     0     0     0     0     0  ...        0   \n",
       "\n",
       "   pix1016  pix1017  pix1018  pix1019  pix1020  pix1021  pix1022  pix1023  \\\n",
       "0        0        0        0        0        0        0        0        0   \n",
       "1        0        0        0        0        0        0        0        0   \n",
       "2        0        0        0        0        0        0        0        0   \n",
       "3        2        2        2        2        2        2        2        2   \n",
       "4        0        0        0        0        0        0        0        0   \n",
       "\n",
       "   pix1024  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        2  \n",
       "4        0  \n",
       "\n",
       "[5 rows x 1025 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(17500, 1025)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_df = pd.read_csv('evaluate.csv')\n",
    "display(eval_df.head())\n",
    "display(eval_df.shape)\n",
    "\n",
    "ID = eval_df['ID']\n",
    "eval_df = eval_df.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nastavím finální model na evaluaci a udělám predikce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.eval()\n",
    "\n",
    "eval_data_tensor = torch.tensor(eval_df.values.reshape(-1, 1, 32, 32), dtype=torch.float).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = final_model(eval_data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = torch.argmax(predictions, dim=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  label\n",
       "0    0      9\n",
       "1    1      4\n",
       "2    2      1\n",
       "3    3      7\n",
       "4    4      1\n",
       "5    5      9\n",
       "6    6      1\n",
       "7    7      6\n",
       "8    8      0\n",
       "9    9      0\n",
       "10  10      8\n",
       "11  11      4\n",
       "12  12      4\n",
       "13  13      3\n",
       "14  14      2"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({'ID': ID, 'label': label})\n",
    "results_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zobrazím několik prvních obrazků s predikovanými labely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(4, 4, figsize=(8,8))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for image_index, ax in enumerate(axs):\n",
    "    image_pixels = eval_df.iloc[image_index].values\n",
    "    # Reshape the pixel values into a 28x28 image\n",
    "    image = image_pixels.reshape(32, 32)\n",
    "    label_image = results_df['label'][image_index]\n",
    "    \n",
    "    ax.imshow(image, cmap='gray', extent=[0, 1, 0, 1])\n",
    "    ax.set_title(f\"Label: {label_image}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uložím náš dataset to csv-souboru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
